{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np  \n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "On a more general level, word2vec embeds non trivial semantic and syntaxic relationships between words. This results in preserving a rich context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Funcion para tokenizar cada tweet \"asd dfg sad\" -> ['asd', 'dfg', 'sad']\n",
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tweet  = str(tweet).lower()\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tokens = filter(lambda t: not t.startswith('@'), tokens)\n",
    "        tokens = filter(lambda t: not t.startswith('#'), tokens)\n",
    "        tokens = filter(lambda t: not t.startswith('http'), tokens)\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'\n",
    "    \n",
    "def tokenizing_and_cleaning(data):\n",
    "    tokens = []\n",
    "    for tweet in tqdm(data[:, 1]):\n",
    "        tokens.append(list(tokenize(tweet)))\n",
    "    tokens = np.array(tokens)\n",
    "    tokens[tokens != 'NC']\n",
    "    return tokens\n",
    "\n",
    "# Etiquetando tokens si corresponden a Training o Test\n",
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "# Construyendo el Word Vector\n",
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding='latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_v = np.array(data)\n",
    "sentiment = data_v[:, 0]\n",
    "text = data_v[:, 5]\n",
    "sentiment[sentiment == 4] = 1\n",
    "tweets = np.c_[ sentiment, text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#n = 10000\n",
    "#tweets = np.concatenate((tweets[:n], tweets[-n:]))\n",
    "tweets = tweets[:1000000]\n",
    "tokens = tokenizing_and_cleaning(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Agregamos los tokens a nuestro array quedando de la forma [sentiment, tweet, token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets = np.c_[ tweets, tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Separamos la data en training y test set y los etiquetamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(tweets[:, 2],tweets[:, 0], test_size=0.2)\n",
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "x_test = labelizeTweets(x_test, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Creamos el vocabulario de palabras, pasando de tokens a vectores por cada palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweet_w2v = Word2Vec(size=200, min_count=10)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm(x_train)])\n",
    "tweet_w2v.train([x.words for x in tqdm(x_train)],total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tweet_w2v.save(\"English_version_w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweet_w2v = Word2Vec.load(\"English_version_w2v_mlp2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Un poco de emellecimiento de los vectores para hacerlos leibles para la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print ('building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in x_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print ('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Train\")\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in (map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Test\")\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in (map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Creamos el modelo y lo entrenamos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Modelo MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               100500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 351,501\n",
      "Trainable params: 351,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, activation='relu', input_dim=200))\n",
    "model.add(Dense(500, activation='relu', ))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "#model.fit(train_vecs_w2v, y_train, epochs=9, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Hora de probar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\"my_model_mlp2.h5\") # Dense(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**MLP1** = loss: 0.3248 - acc: 0.8673 | **MLP2** = loss: 1.0812 - acc: 0.8635 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict_sentiment_one_tweet(tweet):\n",
    "    w2v = list(tokenize(tweet))\n",
    "    w2v = labelizeTweets([w2v], 'TEST')\n",
    "    prediction = model.predict_classes(buildWordVector(w2v[0].words, 200))[0][0]\n",
    "    if prediction:\n",
    "        print(\"Positivo\")\n",
    "    else:\n",
    "        print(\"Negativo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 2874.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "Negativo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tweet = \"How sad and unpresentable that the public television, TVN, did not broadcast the Broad Front debate. A democratic horror.\"\n",
    "predict_sentiment_one_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 1170.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "Positivo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tweet = \"So glad, this is the level of debate we deserve\"\n",
    "predict_sentiment_one_tweet(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "0 : negativo\n",
    "1 : positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 3666.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "Negativo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tweet = \"kast knows nothing\"\n",
    "predict_sentiment_one_tweet(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Nota cuando haga otra prueba necesito recuperar el diccionario de palabras y el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Modelo LSTM (a la noshe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Cambiar nombre diccionario y modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim1 = train_vecs_w2v.shape[0]\n",
    "dim2 = train_vecs_w2v.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=(200,1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "#model.fit(train_vecs_w2v.reshape(dim1, dim2, 1), y_train, epochs=9, batch_size=32, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "translator=Translator()\n",
    "translator.translate(data_v[2][5], src='en', dest='es').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_v = np.array(data)\n",
    "sentiment = data_v[:, 0]\n",
    "text = data_v[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "translator=Translator()\n",
    "spanish_text = []\n",
    "for i in tqdm(text):\n",
    "    spanish_text.append(translator.translate(i, src='en', dest='es').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
